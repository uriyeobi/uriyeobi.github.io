---
layout: post
title: "Biases in logistic regression - it is not about N (Part 1)"
# author: "Chester"
# use_math: true
# comments: true
tags: 'LogisticRegression'
# excerpt_separator: <!--more-->
# sticky: true
# hidden: true
---


Here is a short script I used to run often:

{% highlight python %}
from sklearn.linear_model import LogisticRegression
model = LogisticRegression()
model.fit(X, y)
{% endhighlight %}

But there could be a problem in this naive implemtation of logistic regressions. In this post, we will talk about what it is.

---

<br>


# Logistic regression + Rare events data
Logistic regression is probably the most widely used model (or the first approach one would try) for binary classification problems[^1]. A very simple form and easy to fit. The typical way to estimate it is by maximizing the log likelihood, which is pretty cheap[^2]. 

Now, here is a phrase you might have heard from some STATS101, about the maximum likelihood estimation:

> "Maximum likelihood estimators (MLE) are asymptotically unbiased."

OK, it's good to know, but in practice, who cares? When your data has, say, 1000000 rows, would you really worry about the biasedness of your estimators? 

Well, it depends on the problems, and I guess usually you should be fine as long as you have a decently large number of samples. This is simply because the rate of convergence of MLE is in the order of $N$.

> However, if you're dealing with rare events data, large $N$ may not be enough.

This is because the "rareness" (of events) plays a role, too, for the convergence rate of MLE. More precisely, it is not specifically about "rareness", but more about the actual number of events recorded in your data (we'll see this later)

Still not super clear, so let's check it out directly with numbers.



# Simulations

Here is a toy experiment with synthetic data:

1. Pick sample size: $N \ \ (=10^6)$
2. Pick true parameters: $\beta_0$  and $\beta_1$.
3. Generate `X`: $x_i$ `~ unif[0,1]` ($i=1, ..., N$)
4. Generate `y`: $y_i$ with the probability of events for each $x_i$:
$$P(y_i=1 | x_i, \beta_0, \beta_1) = \cfrac{1}{1 + e^{-\beta_0-\beta_1 x}}$$
4. Run logistic regression estimation with `(X,y)` to get the MLE: $\widehat{\beta_0}$ and $\widehat{\beta_1}$. 
5. Compare ($\widehat{\beta_0}$ , $\widehat{\beta_1}$) with ($\beta_0$ , $\beta_1$).


Here are the results from a few different cases.

<img src="https://github.com/uriyeobi/bias_in_logistic_regression/blob/main/examples/fig1_toy_experiment.png?raw=true" width="430rem">



|  Case  | (average) Probability of Events[^4] | $\beta_0$ |                     <font style="color:0066cc">$\widehat\beta_0$</font>                     | $\beta_1$ |                     <font style="color:0066cc">$\widehat\beta_1$</font>                     |
|:------:|:-----------------------------------:|:---------:|:---------------------------------------------------------------------------------------:|:---------:|:---------------------------------------------------------------------------------------:|
| **#1** |              0.867663               |     1     |                       <font style="color:0066cc">0.996953</font>                        |     2     |                       <font style="color:0066cc">2.005827</font>                        |
| **#2** |              0.098371               |    -1     |                       <font style="color:0066cc">-1.005705</font>                       |    -3     |                       <font style="color:0066cc">-2.993025</font>                       |
| **#3** |              0.003017               |    -4     |                       <font style="color:0066cc">4.032847</font>                        |    -6     |                       <font style="color:0066cc">-5.952327</font>                       |
| **#4** |              0.000101               |    -7     | <mark style="background: #FFB86CA6;"><font style="color:0066cc">-7.117254</font></mark> |    -9     | <mark style="background: #FFB86CA6;"><font style="color:0066cc">-7.933081</font></mark> |
| **#5** |              0.000034               |    -8     | <mark style="background: #FFB86CA6;"><font style="color:0066cc">-8.168674</font></mark> |    -10     | <mark style="background: #FFB86CA6;"><font style="color:0066cc">-8.862332</font></mark> 
{:.type1}



<br>
Note that ($\widehat{\beta_0}$ , $\widehat{\beta_1}$) are fairly close to ($\beta_0$ , $\beta_1$) in **#1**, **#2**, and **#3**, but not in **#4** and **#5**. That is, when the probability of events is very small, the MLEs are biased. We used the same $N(=10^6)$ in all cases, so $N$ shouldn't be solely blamed for the biases. 

Then, is it all about $P$, the probability of events?


# More Simulations

In order to get a better sense of what is going on, let's draw more random numbers along with various configurations.

1. Consider the following combinations of $N$ and $P$:
  * $N$: 10000, 50000, 100000, 500000, 1000000
  * $P$ (probability of events): 50%, 11.2%, 5.73%, 1.17%, 0.59%, 0.12%  
2. In each pair of ($N,P$), repeat the above toy experiment $M$ times, to have $M$ ensembles of MLEs: $\widehat\beta^{(m)}$ ($m=1, ..., M$).
3. Compare $\widehat\beta^{(m)}$ with the true parameter ($\beta$) by the following error metric[^5]: 
$$\textsf{Mean Absolute Relative Error (MARE)}=\displaystyle \sum_{m=1}^M\cfrac{|\widehat\beta^{(m)}-\beta|}{|\beta|}$$

<br>
See below for the results. Clearly, some trends are observed:
* Given $N$, the error increases as $P$ decreases.
* Given $P$, the error levels are higher for smaller $N$. 

<img src="https://github.com/uriyeobi/bias_in_logistic_regression/blob/main/examples/fig2_MC_by_prob.png?raw=true" width="800rem">

<br>
What is more interesting is the next one.

<img src="https://github.com/uriyeobi/bias_in_logistic_regression/blob/main/examples/fig3_MC_by_events.png?raw=true" width="800rem">

This shows the error of the MLE $\widehat\beta_1$ with respect to the number of events. Take a look at the right plot ($\beta_1$) - all the dots are on the same line[^6]. That is, regardless of $N$, it is the actual number of events (call it $N_{events}$) that matters.


# So what?

> So, $N_{events}$, not $N$, determines the biases. 

You'd be better to have many instances of events in your data, **no matter how many rows it has**. From a practical point of view, this is not something pleasant to realize. Yeah, we hear the hype of big data everyday, but many real problems are not big data problems. Think about clinical data collected from patients. Collecting each patient's medical records is usually very tedious or sometimes impossible. So even if you collect such data from a handful of medical institutions, $N$ might be merely several hundreds or thousands at most. What about $N_{events}$ (e.g., number of diagnosed patients)? It should be even much smaller (and we saw from the above simulations that if $N_{events}$ is 100, the relative bias from logistic regression is about 15%, for example).

In many cases where we use logistic regressions for modeling, $N_{events}$ is usually quite small, either because $N$ itself is not large or because $N$ is large but $P$ is very small. Then the biases are already there, from the beginning[^7].

I will stop here for now. In a later post, I will continue the discussion and talk about remedies for the biases.
<br><br><br>



****
**Codes**: [https://github.com/uriyeobi/bias_in_logistic_regression](https://github.com/uriyeobi/bias_in_logistic_regression)

**Notes**

[^1]: To name a few examples of binary signals - [passengers on the Titanic](https://app.box.com/s/rcrahz9pwkctz99y7r5jmnbnz2a1lv9v), [credit card fraud](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud), [loan defaults](https://www.kaggle.com/datasets/yasserh/loan-default-dataset), or [breast cancer](https://www.kaggle.com/datasets/reihanenamdari/breast-cancer).

[^2]: via [Newton's method](https://www.stat.cmu.edu/~cshalizi/350/lectures/26/lecture-26.pdf), since the log likelihood function in this case is convex. Nowadays, it is just a one liner with available packages - [statsmodel](https://www.statsmodels.org/devel/generated/statsmodels.formula.api.logit.html),  [sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) , or [glm](https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/glm).

[^4]: Since $x_i$ `~ unif[0,1]`, the mean of probability of events can be calculated from a direct integration: $E\Big[P(y=1;\beta_0, \beta_1)\Big] =\displaystyle\int_0^1 \cfrac{1}{1 + e^{-\beta_0-\beta_1 x}}dx=\Bigg[\cfrac{\log\big[e^{\beta_0+\beta_1x}+1\big]}{\beta_1}\Bigg]_{x=0}^{x=1}$. For example, with $\beta_0=-1$ and $\beta_1=-3$, this is $\approx 0.098371$.

[^5]: Other metrics could be used, but the big picture wouldn't change.

[^6]: For $\beta_0$, the behavior is similar, but there are some plateau regions.

[^7]: When I was working in a bank, there was a struggling moment when our forecast model (part of which involves logistic regressions) couldn't capture certain corporate credit events properly. This post is partly motivated from that experience.
